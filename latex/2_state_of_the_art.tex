\chapter{State of the Art}\label{cap.estadodelarte}
\setlength{\parindent}{0pt}
\section{Neural Networks in Computer Vision}
Since the birth of Artificial Intelligence (AI) in 1956, the computer vision has followed a great rhythm of evolution. The AI has taking the machines to equal humans in the resolution of some tasks and, in certain cases, to overcome them.
Artificial intelligence is defined in~\cite{mccarthy2006proposal} as ``the subfield of Computer Science dedicated to developing programs that allow computers to present behaviors that can be characterized as intelligent". Machine learning (\textit{ML}) is defined in~\cite{samuel2000some} as ``a field of Computer Science that gives computers the ability to learn without being explicitly programmed". Therefore, given this definition, the ML can be considered a subfield of the AI.\\
One of the most well known and currently growing ML subfields is called \textit{Deep Learning}~\cite{deng2014deep}. This type of algorithm is intimately linked with the \textit{Artificial Neural Networks (ANNs)} and, in practice, they are usually used in an equivalent way although they are not the same. One of the aspects to be highlighted in the Deep Learning algorithms is that it is no longer necessary to extract feature vectors for the input to the machine learning system. This is because these algorithms ``learn" how to represent the data in a hierarchical way. From these networks, the \textit{convolutional neural networks (CNNs)} have a special interest to face the problem that this project presents. This type of networks are characterized by the use of a convolution operation in at least one of the layers of the network and they are designed for the processing of two-dimensional data such as images ~\cite{liu2015implementation}. In recent years, its evolution has been constant being able to overcome the results achieved by previous algorithms in tasks such as object classification and detection.\\

\subsection{Object classification and detection using neural networks}
Much of the progress made in recent years on the classification field of computer vision can be directly associated with a set of neural network architectures. The first big step forward came in 2012 when AlexNet~\cite{krizhevsky2012imagenet} beats all the proposals of the state of the art at that time in the ImageNet challenge, ILSVRC. This competition of classification in images is a reference in the computer vision community. AlexNet obtained a test error rate of 15.3\% compared to the previous year's winner error which was 26.2\%. This network follows the basic design archetype of convolutional neural networks: a series of convolution layers, followed by \textit{max-pooling} and \textit{activation} layers before the final classification layers (\textit{fully-connected}). 
The next architectures are been used as blocks that serve as the basis for numerous subsequent works (commonly known as \textit{backbone networks}) in computer vision and are briefly commented below:
\begin{itemize}
\item \textbf{VGG}~\cite{simonyan2014very}: this architecture from the VGG Group (University of Oxford) makes the improvement over AlexNet by replacing larger kernel-sized filters of size 11 and 5 in the first layers with multiple 3x3 kernel filters one on top of each other. With multiple stacked smaller kernels the depth of the network increases allowing it to learn more complex features at a lower cost.
\item \textbf{MobileNet}: is a simplified version of Xception~\cite{chollet2016xception} for mobile applications that is currently behind the computer vision applications used on Google mobile devices. A year after MobileNet v1, MobileNet v2 was introduced with a great improvement respect to the previous version. For example, the new models used two times fewer operations \cite{sandler2018mobilenetv2}.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.45]{figures/mobilenet_v2.png}
\caption{MobileNet v2 \cite{sandler2018mobilenetv2}}
\label{fig:mobilenet}
\end{center}
\end{figure}
\item \textbf{Inception}~\cite{szegedy2015going}: this family of networks looks for \textit{wider} networks, that is, with more intermediate operations between layers. The authors try to increase neural networks, in terms of operations, without an increase in computational cost. They try to reduce the still huge computational requirements of VGG specially in terms of reducing the number of calculations done due to large width of convolutional layers. Introducing different parallel convolution operations the density of extracted information increases but also the computational costs. To solve the problem they use 1x1 convolutions to reduce dimensionality while performing different transformations in parallel. The resulting networks are simultaneously deep and wide.\\
The first version of Inception, known as GoogLeNet, was the winner of the ILSVRC in 2014. It was improved later with Inception v2 and v3. The last Inception v4 creates a hybrid with ResNet, known as Inception-ResNet~\cite{szegedy2017inception}.
\item \textbf{Res-Net}~\cite{he2016deep}: this network tries to solve the problem that seems to appear when adding layers to a network which is that it generally behaves worse. For this reason, the authors propose that instead of trying to learn the hidden \textit{mapping} of the input \textit{x} to the function \textit{H(x)}, learn the difference between the two, that is, the residue (\textit{residual net}). The original mapping is recasted into \textit{F(x) + x}. This is a big change at the time as it solves the problem of the \textit{vanishing gradients} that the neural networks have suffered until the date. In addition, it allows to create much \textit{deeper} networks, that is to say, with more layers, that will allow better results.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.35]{figures/resnet.png}
\caption{Residual learning block \cite{he2016deep}}
\label{fig:resnet}
\end{center}
\end{figure}
\end{itemize}
With the arrival of autonomous vehicles, intelligent video surveillance, face detection and numerous emerging applications, faster and more accurate detection systems are increasingly in demand. This includes not only recognizing and classifying each object in the image but also obtaining the location with its corresponding \textit{bounding box}. This makes object detection significantly more complicated than traditional image classification. However, the most successful object detection algorithms today are extensions of image classification models. Network architectures such as VGG or ResNet are used as backbone networks for \textit{feature extraction} and after this backbone is added the \textit{head} of the network such as SSD.\\
The main object detection models are now discussed ~\cite{fu2017dssd}.
\begin{itemize}
    \item \textbf{Faster R-CNN}~\cite{ren2015faster}: is one of the current reference models and one of the last detectors known as  \textit{region-based} from Girshick \etal{}This model basically work in the following way: it uses some mechanism to extract regions from an image that are probably an object and then classify those proposed regions with a CNN. The father of this model is the R-CNN and it was the real driver of this type of techniques ~\cite{girshick2014rich}. In the proposed regions obtained through an algorithm called \textit{Selective Search} the characteristics are extracted through a CNN by region and then those regions are classified based on the characteristics. But its performance was slow.\\
    This performance improves with Fast R-CNN~\cite{girshick2015fast} for two main reasons. The first is that the CNN is applied over the whole image instead of over each region and then the regions are obtained from the last map of characteristics of the network. The second is due to the introduction of a \textit{Softmax} layer that simplifies classification. This mechanism was faster and easier to train than R-CNN but there was still a bottleneck in the generation of regions.\\
    To solve it the RPN (\textit{Region Proposal Network}) is introduced and added to the Fast R-CNN to create Faster R-CNN. The RPN returns proposed regions based on a \textit{score} that refers to the probability that the bounding box is an object, the \textit{objectness} (Figure \ref{fig:fasterrcnn}). And these regions are passed directly to the Fast R-CNN to perform the classification.\\
    \begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.3]{previous_version/faster_rcnn.png}
    \caption{Region Proposal Network (from \cite{ren2015faster})}
    \label{fig:fasterrcnn}
    \end{center}
    \end{figure}
\item \textbf{Overfeat}~\cite{sermanet2013overfeat}: winner of the ILSVRC 2013 in location and detection of objects, this work showed that training a convolutional network to simultaneously classify, locate and detect objects in images can enhance the success both in classification, detection and location. Subsequently, it has been replaced by SSD and YOLO for tasks that require better performance in real time.
\item \textbf{SSD}~\cite{liu2016ssd}: it provides great speed gains over Faster R-CNN by performing the phases of generating regions of interest and subsequent classification jointly (\textit{Single Shot MultiBox Detector}). As a result you get a lot of bounding boxes which most of them are not useful. By applying the techniques known as \textit{non-maximum suppression} and \textit{hard-negative mining} the final detections are achieved.\\
In the MobileNet v2 paper \cite{sandler2018mobilenetv2} is proposed \textit{SSDLite} which reduces parameter count and computational cost with respect to regular SSD. To do so, the authors replace all the regular convolutions with separable convolutions.
\item \textbf{R-FCN}~\cite{dai2016r}: there are faster models than Faster R-CNN such as the \textit{Region-based fully convolutional network} or R-FCN. The authors try to solve the problems of SSD for detecting small objects because the detection in SSD was done on the feature map when features have low spatial resolution. This network tries to improve system speed by maximizing shared computing and provides a good balance between speed and accuracy.
\item \textbf{YOLO}~\cite{redmon2016you}: this model, also from the ``single-shot networks family", uses a different approach with respect to the above. This network divides the image into regions and predicts the bounding box and probabilities of each region. These are then weighted with the probabilities to obtain the definitive detections (Figure \ref{fig:yolo}). This performs, as the authors indicate, a hundred times faster than Fast R-CNN maintaining a similar accuracy.\\
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{figures/yolov1.png}
\caption{The YOLO v1 model (from \cite{redmon2016you})}
\label{fig:yolo}
\end{center}
\end{figure}
The YOLO v2 model introduced big improvements: removed all fully connected layers and used anchor boxes to predict bounding boxes, used batch normalization on all convolutional layers and allowed for multi-scale training, among others. In the next table, it can be seen how YOLO v2 is almost on a par with methods like SSD or Faster R-CNN. However, it has a better balance between speed and accuracy since it manages to work in some cases at 91 FPS (\textit{frames per second}) when Faster R-CNN barely reaches 10 FPS (see Table 3 in~\cite{redmon2017yolo9000}).
\begin{table}[H]
\begin{center}
\includegraphics[scale=0.45]{previous_version/yolo_results_pascal12.png}
\caption{Accuracy comparison in test on PASCAL VOC 2012 (from~\cite{redmon2017yolo9000})}
\end{center}
\label{table_yolo}
\end{table}
 The latest version of YOLO, called YOLOv3, achieves a 57,9 mAP on COCO test-dev. The frame rate is lower than the obtained with YOLOv2 (with the same image input size) but it still performs as a state-of-the-art real-time object detection system, according to the author \cite{redmon2018yolov3}.
 \end{itemize}
\subsection{Instance segmentation using neural networks}
The machine vision community has improved the results obtained in object detection and instance segmentation in a short period of time thanks, in large part, to powerful base systems such as Faster R-CNN. This project will use the detections coming from instance segmentation networks so this type of segmentation is going to be introduced, including some recent instance segmentation models such as \textit{Mask R-CNN}.\\ The instance segmentation requires the correct detection of all objects in the image along with the precise segmentation of each instance. Thus, each pixel belongs to one of the different categories without differentiating whether or not it is in a particular object. Semantic segmentation differs from the instance segmentation in that in the first the labels are class-aware whereas in the second the labels are instance-aware.\\
Driven by the effectiveness of the R-CNN family many of the methods proposed for instance segmentation are based on segment proposals where segmentation precedes object type recognition~\cite{pinheiro2015learning}. This has proved to be slower and more inaccurate than if the prediction of object masks and class labels were done in parallel and separately. Li \etal{}propose a system known as FCIS (\textit{Fully Convolutional Instance Segmentation})~\cite{li2016fully} that tries to predict the output of a set of position-sensitive channels in a completely convolutional way. These channels perform the tasks of class, bounding box and masks calculations simultaneously which makes them faster. But it shows errors in instances that overlap creating spurious edges systematically (Figure \ref{fig:fcis_mask}).
Recently, Mask R-CNN~\cite{he2017mask} arose to solve many of these problems and it has situated itself as a state-of-the-art technique in segmentation of instances (see Figure \ref{fig:fcis_mask})\\
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.35]{previous_version/fcis_vs_maskrcnn.png}
\caption{Results obtained by FCIS and Mask R-CNN in test images in COCO Dataset (from \cite{he2017mask})}
\label{fig:fcis_mask}
\end{center}
\end{figure}
Conceptually, Mask R-CNN adds a third stage to Faster R-CNN in which it obtains the mask of the object. The first stage of RPN coincides with that of Faster R-CNN while in the second stage it calculates, in parallel with the prediction of the class and the bounding box, a binary mask for each region of interest (\textit{RoI}). The generation of masks for each class is done without the classes competing with each other, which allows to separate the mask and class predictions from the object prediction. According to the authors, this proves to be the key to obtain good results in the final segmentation.\\ 
Another key factor in the proper functioning of this method is the correct alignment between the RoI and the extracted characteristics. This is usually done using \textit{RoIPool} in Fast R-CNN but introduces misalignments if the purpose is to segment rather than classify. This is why Mask R-CNN authors create \textit{RoIAlign}. To demonstrate the generality of the proposed method the authors introduce the mask prediction branch on several existing neural network architectures such as Faster R-CNN with ResNet, for example, and they manage to surpass the winners of the 2015 and 2016 COCO Challenge segmentation, MNC~\cite{dai2016instance} and FCIS~\cite{li2016fully}.\\
%------------------------------------------------------------------------
\section{Deep learning frameworks in Computer Vision}
For the use of Deep Learning numerous \textit{frameworks} have emerged. A deep learning framework allows us to build deep learning models more easily and quickly, without getting into all the details of the underlying algorithms. In this section the most important frameworks used for deep learning and computer vision tasks are introduced:
\begin{itemize}
\item \textbf{Tensorflow}\footnote {\href{https://www.tensorflow.org/}{Tensorflow}}: developed by Google, it offers a low-level API that allows complete control over model designs and also a more simplified high-level API with limited functionality. For debugging purposes, it provides the Tensorboard tool which allows for the visualization of the model training, among others.
\item \textbf{Keras}\footnote {\href{https://keras.io/}{Keras}}: provides a high-level API for the use of neural networks. Compared to Tensorflow, it offers a more friendly and modular environment which is very interesting when taking the first steps into the deep learning field. As the official Keras documentation indicates it ``is a model-level library" and ``it does not handle low-level operations". For this reason, Keras relies on a optimized tensor manipulation library which serves as ``backend engine". It can run on different \textit{backends} such as Theano or Tensorflow.
\item \textbf{Caffe}\footnote {\href{https://caffe.berkeleyvision.org/}{Caffe}}: Convolutional Architecture for Fast Feature Embedding was originally developed by the University of California (Berkeley). It supports many different types of deep learning architectures orientated towards image classification and image segmentation. It is written in C++ and provides a Python interface. It is quite common to see this type of architecture in terms of programming languages. Due to the speed of C++ compared to Python, C++ is commonly used for deployment environments whereas Python is often used for quick prototyping because of its user-friendly nature.
\item \textbf{PyTorch}\footnote {\href{https://pytorch.org/}{PyTorch}}: is a machine learning library created originally by the Facebook AI research group for the Python programming language. Recently, it has been gaining importance in the ``frameworks' battle". This is mainly due to its tensor computing functionality (similar to \textit{NumPy}) that makes the programming easier.
\end{itemize}
\section{Datasets in Computer Vision}
The datasets used when implementing or testing a certain system are key, since they influence the performance that the system can achieve. They also allow for a comparison of the solution found with respect to others that are part of the State of the Art in the task that is carried out, since they are usually associated with some type of competition. Therefore, it is necessary to correctly choose the dataset or datasets used in a computer vision problem. Here are some of the most well-known datasets used in many applications in the computer vision field:
\begin{itemize}
\item \textbf{COCO (Common Objects in Context)}\footnote{\href{http://cocodataset.org/#home}{COCO Dataset}}: it is a large scale dataset for detection and segmentation of objects mainly. It contains 80 categories of objects and 330000 images of which more than 200000 are labeled. It is a dataset widely used between the community and in congresses such as the ICCV\footnote{\href{http://iccv2019.thecvf.com/}{ICCV}} (International Conference on Computer Vision).
\item \textbf{PASCAL VOC}\footnote {\href{http://host.robots.ox.ac.uk/pascal/VOC/}{PascalVOC Dataset}}: this dataset is linked with another challenge, the Pascal VOC Challenges. They ran this competition from 2005 to 2012. This project provides standardised image datasets for object class recognition, segmentation or action classification tasks.
\item \textbf{ImageNet}\footnote {\href{http://www.image-net.org/}{ImageNet Dataset}}: it consists of 14 million images approximately and an average of 500 images per category. It organizes the well-known ILSVRC\footnote{\href{http://image-net.org/challenges/LSVRC/}{ILSVRC}} competition (ImageNet Large Scale Visual Recognition Challenge) of location and detection of objects in images and videos. It is one of the reference datasets in this area.
\item \textbf{KITTI}\footnote {\href{http://www.cvlibs.net/datasets/kitti/}{KITTI Dataset}}: centered in the autonomous driving field, this vision benchmark suite introduces itself as a novel challenging real-world computer vision benchmark. The main areas of interest include 3D/2D object detection, 3D tracking or stereo vision. The type of objects for object detection available are focused in the ADAS field such as car, van, truck, pedestrian or cyclist.
\item \textbf{Cityscapes}\footnote {\href{https://www.cityscapes-dataset.com/}{Cityscapes Dataset}}: this dataset focuses on semantic segmentation in urban scenes. It contains 30 kinds of objects, 5000 images labeled with a \textit{fine} label (more precise) and 20000 labeled with a \textit{coarse} label in 50 different cities.
\item \textbf{OpenImages}\footnote {\href{https://storage.googleapis.com/openimages/web/index.html}{OpenImages Dataset}}: it is a dataset of about 9 million images. This makes it the ``largest existing dataset with object location annotations". It also has a bigger number of classes than other challenges as the previously cited COCO and PASCAL VOC, exactly 600 object classes. It must be mentioned that the label distributions are usually \textit{skewed} and with OpenImages it ocurres too. This means that there are many more objects of some kinds than others.
\end{itemize}
There are many other datasets such as those from research centers like INRIA, MIT or Caltech that contribute to the continuous improvement of the computer vision data.
\section{Object tracking}
\subsection{Datasets}
Apart from these it is necessary to talk about the datasets that are focused on the core part of this work, i.e.\ \textit{the visual object tracking}. The visual object tracking is a fundamental task in computer vision which has importance in many applications such as surveillance, autonomous vehicle or video analysis. This task, the same way as others in the field needs datasets from which create and evaluate the algorithms. The datasets used are also commonly associated with competitions that allow the benchmarking of the developed algorithms. This benchmarks often provide the most objective measure of performance and, for this reason, they are important guides for research in the area of study.

The visual tracking datasets are going to be divided according to their \textit{tracking target}, that is, if they are focused on the tracking of a single object (SOT) or on the tracking of multiple objects (MOT).
\subsubsection{Multiple object tracking datasets}
\begin{itemize}
\item \textbf{MOT} ~\cite{milan2016mot16}\\
This dataset arises from the need to provide a general and standardized way to create multi-object tracking algorithms, evaluate the results and present them. In the recent past, the computer vision community has promoted several benchmarks for the evaluation of numerous tasks such as object detection, optical flow or stereo estimation that have advanced the state of the art in these areas. However, not so much effort has been made in the standardization of the evaluation of multiple target tracking.\\
As many other datasets it is associated with a challenge, the \textit{MOTChallenge}. With this challenge they try to create a unified framework for the evaluation of multi-target tracking. The dataset provides a collection of datasets, some of them coming from datasets already in use and some from new challenging data. The given data are video sequences as always occurs when working in tracking tasks.\\
The first release of the dataset named \textit{MOT15} was focused on multiple people tracking, following the tendence of other datasets. The pedestrian tracking is by far the most studied case in the tracking context. In the next releases, more significant classes generally seen in urban scenarios were added like vehicles, bicycles or motorbikes. The challenge has had three editions: \textit{MOT15, MOT16, MOT17}. In each of them the sequences were more challenging than the edition before. This can include different camera viewpoints and positions, more challenging weather conditions (cloudy, night, sunny). For example, the mean crowd density in MOT16 is three times higher when compared to the first benchmark release.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{previous_version/mot16.png}
\caption{An overview of the MOT16 dataset. Top: raining sequences. Bottom: test sequences ~\cite{milan2016mot16}}
\label{fig:mot}
\end{center}
\end{figure}
\item \textbf{ALOV}\footnote{{\href{http://alov300pp.joomlafree.it/dataset-resources.html}{ALOV Dataset}}}\\
The \textit{Amsterdam Library of Ordinary Videos for tracking} is another well-known visual object tracking dataset in the field. It aims to cover as diverse circumstances as possible including illuminations, transparency, zoom or low contrast, for example. The dataset consists of 315 video sequences mainly obtained from YouTube with 64 different types of targets. The sequences are normally short with an average length of 9.2 seconds and the total number of frames is 89364 (in ALOV300).
\item \textbf{CAVIAR} \cite{dubuisson2016survey}\\
The CAVIAR project (\textit{Context Aware Vision using Image-based Active Recognition}) from INRIA labs was dedicated to the development of algorithms that can describe and understand video scenes. The scenes were associated with surveillance scenarios where people performed some different activities related with the surveillance area. Those activities included \textit{walking}, \textit{browsing}, \textit{resting}, \textit{leaving bags behind} or \textit{two people fighting}. The annotations contain, apart from the bounding boxes locations, the head and feet positions, the body direction, among others. Refering to the tracking task, the challenging problems include occlusions, appearance/disappearance, appearance changing or similar object tracking, for example. In terms of data size, the first set contains 28 video sequences and the second set contains 44 video sequences (Figure \ref{fig:caviar}). It is a well-known dataset and is commonly used for development and testing of tracking algorithms.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{previous_version/caviar.png}
\caption{CAVIAR: From  left  to right, two frames (ground truth superposed) from sequences of datasets 1 (entrance lobby of INRIA Labs) and 2 (hallway of a shopping center) ~\cite{dubuisson2016survey}}
\label{fig:caviar}
\end{center}
\end{figure}
\item \textbf{BEHAVE} \cite{dubuisson2016survey}\\
Similarly to CAVIAR dataset, the BEHAVE Interactions Test Case Scenarios dataset contains various video sequences with different scenarios where people perform different interactions among which are \textit{walk together}, \textit{meet} or \textit{split} (Figure \ref{fig:behave}). The annotations include labels in case of interactions. Proposed for behavior analysis of interacing groups, this dataset was also used for other purposes like the validation of visual tracking algorithms that consider occlusions or fast and varying motion of objects.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.38]{previous_version/behave.png}
\caption{BEHAVE: Two snapshots of sequences, with the ground truth bounding boxes of the objects to track \cite{dubuisson2016survey}}
\label{fig:behave}
\end{center}
\end{figure}
\item \textbf{PETS} \cite{dubuisson2016survey}\\
The \textit{International Workshop on Performance Evaluation of Tracking and Surveillance} organizes a visual tracking competition with different objectives on every edition starting from 2000. In 2013\footnote {\href{http://www.cvg.reading.ac.uk/PETS2013/index.html}{PETS 2013}}, two of the objectives were the tracking and counting of people in crowds to estimate the density and detecting events by crowd analysis. As BEHAVE or CAVIAR, PETS datasets are very popular among the computer vision community. The latest PETS edition took place in 2017\footnote {\href{http://openaccess.thecvf.com/content_cvpr_2017_workshops/w34/papers/Patino_PETS_2017_Dataset_CVPR_2017_paper.pdf}{PETS 2017}} and continued the evaluation theme of on-board surveillance systems for protection of mobile critical assets started in PETS 2016 \cite{patino2016pets}. On this edition, the dataset included sequences that adressed the protection of trucks (Figure \ref{fig:pets}) or vessels at sea, among others.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.27]{previous_version/pets2016.png}
\caption{PETS 2016: A group of people detected and tracked walking by a truck \cite{patino2016pets}}
\label{fig:pets}
\end{center}
\end{figure}
\item \textbf{TrackingNet} \cite{muller2018trackingnet}\\
Most of the commented datasets are limited by its small size. Even more if they are going to be used by data-hungry trackers based on deep learning. Currently, this trackers rely on object detection datasets due to the lack of dedicated large-scale tracking datasets. For this reason, the authors created TrackingNet, the first large-scale dataset and benchmark for object tracking in the wild. TrackingNet provides a total of 30643 video segments with more than 14 million dense bounding box annotations (Figure \ref{fig:trackingnet}). The contributions of this work include different techniques to generate dense annotations from coarse ones and an extended baseline for state-of-the-art trackers benchmarked on TrackingNet. Referring to the latter, the authors affirm that pretraining deep models on this dataset can improve their performance on other datasets by increasing their metrics by up to 1.7\%.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{previous_version/trackingnet.png}
\caption{TrackingNet: Comparison of current datasets size for object tracking \cite{muller2018trackingnet}}
\label{fig:trackingnet}
\end{center}
\end{figure}

\subsubsection{Single object tracking datasets}
\item \textbf{OTB} ~\cite{wu2013online}\\
There exist several datasets for visual tracking in surveillance escenarios but often the target objects are humans or cars of small size with a static background. Also, some of the scenes are sometimes not annotated with bounding boxes which makes them not very useful for the comparison of tracking algorithms. To facilitate the evaluation task the authors built a tracking dataset with 50 fully annotated sequences in the first release \textit{OTB50}. Later, the dataset was extended with another 50 sequences (\textit{OTB100}).\\ Many factors can affect the tracking performance such as illumination variation or occlussion, for this reason the authors categorized the sequences with 11 attributes according to the occurrence of any of the selected factors (Figure \ref{fig:otb}).\\
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{previous_version/otb_attributes.png}
\caption{OTB: List of the attributes annotated to test sequences ~\cite{wu2013online}}
\label{fig:otb}
\end{center}
\end{figure}
Apart from the data side, the authors also integrated most of the publicly available trackers at the time to create a code library with uniform input and output formats to facilitate large scale performance evaluation. Including TLD \cite{kalal2010pn}, MIL \cite{babenko2009visual} or CPF \cite{perez2002color} making a total of 29 tracking algorithms.
\item \textbf{VOT} \cite{kristan2017visual}\\
The \textit{Visual Object Tracking} initiative started in 2013 to address performance evaluation of short-term visual object trackers. The short-term tracking means that trackers are assumed to not be capable of performing successful re-detection after the target is lost and they are therefore reset after such event. In all the previous editions the challenge considers single-camera, single-target, model-free\footnote{The only training information provided is the bounding box in the first frame}, causal trackers\footnote{The tracker does not use any future frames, or frames prior to re-initialization, to infer the object position in the current frame}, applied to short-term tracking. The main goal of VOT is establishing datasets, evaluation measures and toolkits for visual object tracking (as many other initiatives). The successive editions were made in conjunction with Computer Vision Conferences like ICCV or ECCV. In 2015, a subchallenge focussed on tracking in thermal infrared (\textit{TIR}) was made due to the growing interest in this kind of imaging. The 7th Visual Object Tracking Challenge VOT2019 workshop will be held in conjunction with the ICCV2019. With respect to the previous edition in 2018, this challenge edition introduces the evaluation of trackers that use 4 channels (\textit{RGB-IR} and \textit{RGB-depth}).\\
Referring to the data itself, the VOT datasets try to pay more attention to the diversity of the data and the quality of the content and annotation with respect to the quantity. For example, some datasets assign a global attribute to the entire sequence when it is happenning in a fragment of it. VOT dataset tries to avoid the assumption that the quality of the data is correlated with its size. The VOT Challenge has focused on developing a methodology for automatic construction and annotation of moderately large datasets from a  large pool of sequences (Figure \ref{fig:vot}). For example, they use sequences from datasets such as the OTB.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{previous_version/vot.png}
\caption{VOT: Images from the VOT2016 sequences (left column) that were replaced by new sequences in VOT2017 (right column) ~\cite{kristan2017visual}}
\label{fig:vot}
\end{center}
\end{figure}
\item \textbf{Need for Speed} \cite{kiani2017need}\\
Visual object tracking algorithms have been usually evaluated at the canonical frame rate of 30 frames per second but consumer devices with cameras such as smartphones, tablets or drones are increasingly coming with higher frame rates. This can take time for the visual object track community to adapt to what \textit{real time} means in terms of how faster frame rates affect the choice of a tracking algorithm. The authors introduce Need for Speed (\textit{NfS}) as the first higher frame rate video dataset and benchmark for visual object tracking. The dataset consists of 100 videos captured with 240 FPS cameras from real world scenarios. The frames are annotated with bounding boxes and the sequences are manually labelled with nine visual attributes (occlusion, fast motion, etc.). The work also provides a ranking of many recent state-of-the-art trackers according to their tracking accuracy and real-time performance. One interesting conclusion the authors obtained is that at higher frame rates, simple trackers such as correlation filters outperform complex methods based on deep learning. This must be taking into account when making the choice of a tracking algorithm in practical applications. It needs to be a tradeoff between the resources (available bandwidth, computation hardware, etc.) and the required application accuracy.
\end{itemize}

\subsection{Previous work on tracking}
Once the data on which the tracking algorithms can be built it is necessary to start talking about such algorithms. Therefore, in this section we will talk about object tracking methods.\\
The main objective in object tracking is to estimate the state of the object (\textit{target}) over the time in a series of sequences of images (\textit{frames}). This state can be defined by different characteristics such as shape, appearance, position or speed.\\
It is a difficult field since one or more circumstances can be given that must be solved by the algorithm. Among them are the management of variations in lighting and in the point of view of the object that can lead to changes in the appearance of the same. Likewise, the occlusions that occur when objects are mixed with other elements of the scene or the quality of the image itself may be a problem to consider in this area.\\
To confront these problems the following paradigms have been followed~\cite{smeulders2014visual}:
\begin{itemize}
\item \textbf{Tracking using matching}: this group of algorithms makes a \textit {matching} between the representation of the model of the object created from the previous frame and the possible candidates in the next frame. This methods rely on the correct representation of the match and the similarity measurement used to perform the matching. The most outstanding methods are \textit{Normalized Cross-Correlation}~\cite{briechle2001template}, \textit{Lucas-Kanade Tracker}~\cite {baker2004lucas}, \textit{Kalman Appearance Tracker}~\cite{nguyen2004fast} and \textit{Mean Shift Tracking}~\cite{comaniciu2000real}.
\item \textbf{Tracking-by-detection}: a model is built to distinguish the object from the background~\cite{nguyen2006robust}. Once you have the detection it is associated with the other detections. Currently, the community is turning to neural networks to compute detections.
\item \textbf{Tracking, learning and detection}: it is an extension of the previous group that includes a mechanism to update the model that is \textit{learned} during execution. For example, you can use the results of an \textit{optical flow} tracker for this update ~\cite{kalal2010pn}. This ensures that the algorithm is invariant to changes in the object.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{figures/pn_learning.png}
\caption{P-N learning mechanism (from~\cite{kalal2010pn})}
\label{fig:pn}
\end{center}
\end{figure}
\end{itemize}
Prior to the modern techniques to be discussed there are more ``classic ways" of tracking objects that can be useful in problems that require real time, for example. One of the most well-known is \textit{feature tracking}. This technique uses characteristic points that can be found in images and that allow to estimate the movement. These points must meet some requirements to be able to be characteristic of the image such as \textit{repeatability} (the characteristic can be found in the images even if they have undergone some transformation), \textit{compatibility} (each characteristic must be descriptive and easy to find) or \textit{efficiency} (the representation of the information characteristic of the image must be done with as few characteristics as possible). Some of the characteristic points most commonly used are \textit{corners}. They are characterized by gradients with higher values in them in two or more directions. These techniques can be seen in Harris~\cite{harris1988combined} and Shi-Tomasi corner detectors~\cite{shi1994good}.\\
There are tracking systems that take advantage of the speed of feature tracking and the accuracy of neural networks to create a ``hybrid tracking". In this type of tracking the detections are done each N frames using some type of neural network and the intermediate tracking is done through feature tracking.\\
With the arrival of neural networks this way of grouping the different tracking methods changes to adapt to them~\cite{held2016learning}:
\begin{itemize}
\item \textbf{Tracking-by-detection}: they are designed to follow a certain class of object (\textit{model-based}) and to obtain a specific classifier. In practice, the detections are obtained with neural networks and they are linked in tracking using temporal information. They are limited to a single class of objects.
\item \textbf{Tracking, learning and detection}: they are characterized by being fully trained \textit{online}. A typical tracker example of this group samples zones close to the object and considers them \textit{foreground}, the same happens with the distant zones that would be assigned to the \textit{background}. With this a classifier can be built that differentiates them and estimates the new location of the object in the following frame~\cite{babenko2009visual}. It has been tried to introduce neural networks in environments with online training but due to the slowness of the networks when training the results are slow in practice.
\item \textbf{Siamese-based tracking}: this type of networks use \textit{patch-matching} techniques ~\cite{tao2016siamese}.   Multiple patch candidates from the new frame are received and the one with the highest \textit{matching score} with respect to the previous frame is chosen as the best candidate, that is, the most similar according to the matching function.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{figures/siamese.png}
\caption{SiamRPN++ (from~\cite{li2018siamrpn++})}
\label{fig:pn}
\end{center}
\end{figure}
In the figure above, one of the last siamese network based tracker called \textit{SiamRPN++}. This network, state-of-the-art in visual object tracking on VOT2017/18 and accepted to participate on CVPR 2019, follows the commented strategy which formulates tracking as convolutional feature cross-correlation between a target template and a search region.
\item \textbf{Tracking as regression}: in this group, on the other hand, the network receives only two images (the previous frame and the current one) and directly returns the location of the object in the current frame. Since this tracker predicts a bounding box instead of just the position, it is able to model changes in scale and aspect of the tracked template. However, it only can process a single target and it needs from data augmentation techniques to learn all possible transformations of the the targets~\cite{held2016learning}.
\item \textbf{Tracking with RNN}: this type of algorithms use \textit{Recurrent Neural Networks} to model the sequence of movement of objects from the detection obtained. Thus improving the response to prolonged occlusions in time, for example ~\cite{sadeghian2017tracking}. They are the state of the art in tracking nowadays in terms of accuracy but they usually do not perform in real-time.
\end{itemize}
\section{Metrics}
Apart from the datasets and algorithms used to solve a given task or problem it is necessary to use a measure or measurements that can provide an evaluation of the performance of the obtained solution. In this section, the most important metrics used for evaluating object detection and multiobject tracking are commented.
\subsection{Metrics for the evaluation of object detection algorithms}
Before going deeper with the most common metrics in the evaluation of object detection, the basic concepts need to be mentioned. When talking about object detection, the following definitions usually appear:
\begin{itemize}
    \item \textbf{Intersection over Union (IoU)}: also known as Jaccard index, this measure evaluates the overlap between two bounding boxes, a predicted bounding box and the ground truth bounding box. With this definition a prediction can be classified into valid (TP) or invalid (FP). See Figure \ref{fig:iou}.
    \begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.5]{figures/iou.png}
    \caption{IoU definition (from \href{https://github.com/rafaelpadilla/Object-Detection-Metrics#intersection-over-union-iou}{link})}
    \label{fig:iou}
    \end{center}
    \end{figure}
    \item \textbf{True Positive (TP)}: is a correct detection. The condition is that the IoU must be above or equal to a given threshold. This threshold is usually defined in percentage to 50\%, 75\% or 95\%. The results obtained by a system with these three thresholds can define its behavior. For example, a given object detector can easily have good results at a 0,5 IoU but not so easily at a 0,95 IoU.
    \item \textbf{False Positive (FP)}: is a false detection. The IoU of the detection must be below the threshold.
    \item \textbf{False Negative (FN)}: is a detection not detected.
    \item \textbf{True Negative (TN)}: it is not important but it is defined as all the possible bounding boxes that were correctly not detected. It is not used in metrics.
\end{itemize}

It is very common to see that the metrics are established by a given challenge or associated with it. It is the case of the Pascal VOC challenge that uses the \textit{precision/recall curve} and the \textit{average precision}. These terms are now defined:
\begin{itemize}
    \item \textbf{Precision}: is the proportion of correct positive predictions.
    \begin{equation}
        Precision = \frac{TP}{TP + FP}
    \end{equation}
    \item \textbf{Recall}: is the proportion of positive predictions with respect to all positives.
    \begin{equation}
        Recall = \frac{TP}{TP + FN}
    \end{equation}
    \item \textbf{Precision/Recall curve}: this curve plots the performance of an object detector as the confidence is changed for each object class. A good precision/recall curve has a high precision while recall increases, i.e.\ if the confidence threshold varies, the precision and recall stay high.
    \item \textbf{Average precision (AP)}: the AP summarizes the shape of the previous curve allowing to obtain the \textit{Area Under the Curve (AUC)}. This is done because of the nature of the precision/recall curve in form of ``zigzags"  that does not permit an easy comparative between different curves (detectors). This numeric metric is the precision averaged across all recall values between 0 and 1.
    \begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.6]{figures/auc.png}
    \caption{AUC example: the areas from the trapezoids are 0,335, 0,15875 and 0,1375 respectively, giving an AUC score of 0,63125 (from \href{https://classeval.wordpress.com/introduction/introduction-to-the-precision-recall-plot/}{link})}
    \end{center}
    \end{figure}
    This average can be done in two main ways: 11-point interpolation or interpolating all points.
    \begin{itemize}
        \item \textbf{11-point interpolation}: is defined as the mean precision at a set of eleven equally-spaced recall values ranging from 0 to 1. The precision at each recall value is obtained by taking the maximum precision measured value for a method for which the corresponding recall is above \textit{r}.
        \begin{figure}[H]
        \begin{center}
        \includegraphics[scale=0.7]{figures/interp1.png}\\
        \includegraphics[scale=0.7]{figures/interp2.png}
        \caption{11-points interpolation (from \href{https://github.com/rafaelpadilla/Object-Detection-Metrics#11-point-interpolation}{link})}
        \end{center}
        \end{figure}
        \item \textbf{All points interpolation}: in this case the mean precision is done interpolating through all recall points. The precision at each level \textit{r} is obtained now taking the maximum precision which has a recall value greater or equal than \textit{r + 1}. From 2010 on PASCAL VOC uses this method of interpolation.
        \begin{figure}[H]
        \begin{center}
        \includegraphics[scale=0.7]{figures/interp3.png}\\
        \includegraphics[scale=0.7]{figures/interp4.png}
        \caption{All points interpolation (from \href{https://github.com/rafaelpadilla/Object-Detection-Metrics#interpolating-all-points}{link})}
        \end{center}
        \end{figure}
        
    \end{itemize}
\end{itemize}

\subsection{Metrics for the evaluation of multiobject tracking algorithms}
For the metrics used in evaluation of multiple object tracking algorithms the classification from MOT \cite{milan2016mot16} is used as reference. As it will be seen the performance evaluation for MOT algorithms is not so straightforward as the one presented for object detection.\\
The metrics for tracking can be classified into four subsets according to different attributes:
\begin{itemize}
    \item \textbf{Accuracy:} this type of metrics try to measure how accurately a tracking algorithm can track targets. From this type of metric the following two are briefly commented: \textit{IDs} \cite{yamaguchi2011you} and \textit{MOTA} \cite{bernardin2008evaluating}.\\ The IDs metric measures the ID \textit{switches}, i.e.\ given an id for an object it measures how many times the MOT algorithm changes this id.\\ The \textit{Multiple Object Tracking Accuracy} or MOTA is calculated as follows:
    \begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.3]{figures/mota.png}
    \caption{MOTA definition: where $m_t$ is the number of misdetections (FN), $fp_t$ is the number of false positives (FP), $mme_t$ is the number of mismatches (IDs) and $g_t$ is the sum of TP and FN (from \cite{bernardin2008evaluating})}
    \label{fig:mota}
    \end{center}
    \end{figure}
    This combination of FP rate, FN rate and mismatch rate into a single number is, as the authors indicate, ``by far the most widely accepted evaluation measure for MOT" \cite{milan2016mot16} and it gives an intuitive measure on the tracker's performance at detection and trajectory. It does not take into account the precision of that detections' location.

    \item \textbf{Precision:} in this metrics group the key factor is the description of the precision that the tracked objects have using criteria such as bounding box overlap or distance. The most important are \textit{MOTP} \cite{bernardin2008evaluating}, \textit{TDE} \cite{kratz2010tracking} and \textit{OSPA} \cite{ristic2011metric}. MOTP, for example, uses a ratio with the distance between the ground-truth detections locations \textit{d} and the associated detected locations \textit{c}.
    \begin{figure}[H]
    \begin{center}
    \includegraphics[scale=0.4]{figures/motp.png}
    \caption{MOTP definition (from \cite{bernardin2008evaluating})}
    \label{fig:motp}
    \end{center}
    \end{figure}
    \item \textbf{Completeness:} it refers to how completely the ground truth trajectories are tracked. This set includes the results from \textit{Mostly Tracked (MT), Partly Tracked (PT), Mostly Lost (ML)} and \textit{Fragmentation (FM)} \cite{li2009learning}.
    \item \textbf{Robustness:} this last type of metrics are linked to the recovering from occlusion. Examples of this group are \textit{Recover from Short-term occlusion (RS)} and \textit{Recover from Long-term occlusion} \cite{song2010stochastic}.
\end{itemize}
