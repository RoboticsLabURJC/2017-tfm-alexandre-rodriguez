\chapter{Multiobject tracking using deep learning and tracking by detection}
In this chapter, the solution obtained for solving the multiobject tracking problem using deep learning and tracking-by-detection is explained.

\section{Design overview}
The main contribution of this work is to develop a tracking algorithm capable of tracking different types of objects using deep learning techniques. To achieve this task the selected tracking methodology is the tracking by detection. Our method combines detections coming from an object detection neural network with tracking techniques. With this, the idea is to give the final system a balance between speed and accuracy. The detections from the neural networks are usually slower than a pure tracking but more accuracted whereas the tracker results are often quickly obtained but they are slightly more inaccurated.\\
The module architecture of the system is summarized in the following diagram (Figure \ref{fig:general}).
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{figures/general.png}
\caption{modular architecture}
\label{fig:general}
\end{center}
\end{figure}

As it can be seen the system is built in a modularized way with different threads. There are 4 threads: Camera, GUI, Network and Tracker. All of them are going to be discussed more in deep on its respective sections of the chapter but the general workflow of the system is going to be explained here (Figure \ref{fig:update_cam}).\\
First, the camera thread provides the images or frames to the rest of the threads, i.e.\ the input to the system. The output of the system can be provided using an YML file with the results per frame (\textit{Logger}) or using the GUI. If the GUI is configured to show the graphical interface (\textit{on}), the results are shown on screen. But if the graphical interface is not configured (\textit{off}) the results are saved in JPG files.\\ The core of the computing is divided in the Network thread and the Tracker thread. This Tracker has a buffer of frames of different size coming from the Camera to work on \textit{delayed real-time}. So, when the first frame is available it is given to the Network thread which starts doing the inference, this is, it starts detecting objects. Meanwhile, the buffer is accumulating the incoming frames from the Camera until the detection from the Network comes.\\
When the detection is available, the Tracker thread starts the tracking of the detected objects in the buffer of frames. The last frame in the buffer is used to feed again the Network allowing for a synchronism between the detections and the tracking in the frames. This temporal process can be observed more clearly in the Figure \ref{fig:buffer}.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{figures/buffer.png}
\caption{How the buffer is handled}
\label{fig:buffer}
\end{center}
\end{figure}
It can be seen that given a complete buffer in Camera of size \textit{N}, the first frame is assigned to the Network and, when this frame is processed, a buffer of size \textit{N-2} is assigned to the Tracker to be processed. The last frame of the given buffer is passed to the Network again. This iterative mechanism continues in time ensuring that no frames are lost. The application has always a delay in frames of 1 buffer (\textit{delayed real-time}).
\\As commented before, the buffer changes its size in every iteration but we can not allow the buffer to increase or decrease this size in an uncontrolled way because that will end blocking the system. If the buffer is too big, the tracking will take more time and the neural network will finish before the tracking is done. This is, the Network is underused. In the other side, if the buffer is too small, the Tracker will end its work before the inference is done in the Network and the Tracker will have to wait much more time to the Network to finish. For these reasons a balance is needed.\\
The neural network inference time is approximately the same time so this time is fixed. Then, the only part of this processing core where we can change is on the Tracker side. The obtained solution consists of a Tracker which constantly measures its frame rate (FPS) allowing it to slow down or speed up depending on this measurement.\\
Once all the frames have been processed the Network and Tracker thread stop. After that, the results are logged into the YML files (a file for each frame) and the user can close the application.\\
The general behavior of the application when running video sequences or raw frames was presented. Nevertheless, it can also handle live stream videos coming from local cameras connected. In this case, the logging of the results is not done but the tracking by detection scheme is the same.\\
The ``main" is done in the \texttt{update} function of the Camera which is continuously called by the Camera thread. As the system architecture is based in multiple threads, the synchronism between them is crucial. Because of this, the control of the application is done taking into account the synchronism between all the threads, their internal status and variables.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.3]{figures/update_camera.png}
\caption{The control flow}
\label{fig:update_cam}
\end{center}
\end{figure} 

As commented in the chapter 2, the application is configurable using an YML file (\textit{objecttracker.yml}). For the instructions on how to run the application please refer to the wiki of the project (see \ref{Methodology}).
\\In the next sections, the implementation of each thread of the system is explained, including the available configuration modifications.\\

\section{Video source}
The Camera module provides the input images to the rest of the system. This images can be obtained using four different sources:
\begin{itemize}
    \item \textbf{Local camera (with OpenCV)}: the Camera can read from a local camera using the OpenCV routine \texttt{VideoCapture} indicating the device number of the camera.
    \item \textbf{Local camera (with ROS)}: the Camera can use ROS to read from the camera device. In order to do so, the user needs to launch a terminal and type \texttt{roslaunch usb\_cam.launch}. This will publish a ROS topic \texttt{/usb\_cam/image\_raw} that can be subscribed by the Camera module and it will start reading frames from the device. For more information about the launching process please refer to my wiki (\ref{using_ros}).
    \item \textbf{Local video}: to read from a local video it is also used the OpenCV routine \texttt{VideoCapture} but indicating the video path.
\item \textbf{Local images}: similarly the local images path needs to be passed to \texttt{VideoCapture}. This source is very useful because most of the datasets are provided as sequences of frames instead of videos. And it can avoid problems such as creating sequences of videos with the wrong duration or frame rate.
\end{itemize}
The Camera thread source and options can be modified at the configuration file. The source is selected at \texttt{ObjectTracker->Source}. After that the user needs to indicate the device number \texttt{ObjectTracker->Local->DeviceNo} if using a local camera with OpenCV, the video path \texttt{ObjectTracker->Video->Path} if using a local video or the images path \texttt{ObjectTracker->Images->Path} when using local images.\\
The user needs to modify the \texttt{usb\_cam.launch} to change the Camera configuration when using ROS.\\
Before being sent to other threads the image is rescaled according to the neural network input size. It continues all the process with this size. When the final results are obtained this is taken into account to rescale the coordinates of the detections or trackers with respect to the original image size.\\
Apart from providing images the Camera thread also controls the flow of the application. This is done in this way because the application offers the option of not having GUI. In the first versions of the project the control of the application (the ``main") was implemented in the GUI thread but when the no-GUI option was added this was moved to the Camera thread.\\

\section{GUI}
The GUI module provides the interface with the user and, as commented before, is optional. It was implemented using the tools provided by PyQt5, in concrete with the packages \textit{QtGui, QtCore} and \textit{QtWidgets}.\\
The graphical interface has four windows and two buttons. The top-left window shows the input frames in real-time while the top-right one shows the final results. The intermediate results obtained from the Network and the Tracker will be provided at the bottom. The application with GUI has two modes with its respective buttons: ``run continuous" and ``run now". In the first one, the application runs continuously until it finishes the process (the process finishes depending on the image source). In the second one, the user can push the \textit{Run now} button to make the Network detect on the current frame and continue the tracking from that frame towards.
\section{Neural Network} 
The Network module is tasked with the object detections in the images which are feeding the Tracker on every iteration. It supports Tensorflow and Keras object detection models. The Tensorflow models can be obtained from the Tensorflow detection model zoo\footnote{\href {https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md}{Tensorflow detection model zoo}} and include SSD and R-CNN detectors. The Keras models are limited to SSD architectures and it can be obtained from this set\footnote{\href {https://github.com/pierluigiferrari/ssd_keras#download-the-original-trained-model-weights}{Keras models}}.\\
The datasets on which the model was trained need to be specified to assign the labels to the objects. The supported labels include the VOC, COCO, KITTI, OID and PET datasets. As it ocurred with the Camera module the Network module has configurable options available in \textit{objecttracker.yml}:
\begin{itemize}
    \item \texttt{Framework}: Keras or Tensorflow
    \item \texttt{Model}: the model file
    \item \texttt{Dataset}: VOC/COCO/KITTI/OID/PET
    \item \texttt{Input size}: this input size can be modified depending on the selected model. Some models does not allow to change the input image size
    \item \texttt{Confidence}: the confidence threshold for the detections obtained. If a detection obtains a confidence value below that detection is discarded
\end{itemize}
The Network thread basically receives an image (previously resized) and performs the inference. As a result it outputs the detections obtained in the image (if any) and it draws these detections in form of bounding boxes containing also the label and the confidence value.\\
The logging of the Network and Tracker results are optional and it can be changed at the YML configuration file in \texttt{ObjectTracker->Logger->Status}. These results are logged in the \texttt{logNetwork} and \texttt{logTracker} functions respectively.\\

\subsection{Tensorflow models}
These models are obtained from the Tensorflow detection model zoo which provides models for inference out-of-the-box, i.e.\ to be directly used. From the models available only some of them were tested for its use in the project. The pre-trained models used were trained on the COCO dataset because the classes available in this dataset were considered enough for the type of objects that can be seen in the tracking sequences used. In the next table, the Tensorflow models performance can be seen:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Model name                                 & Speed (FPS) & COCO mAP \\ \hline
\textbf{ssd\_mobilenet\_v2\_coco}          & 32         & 22       \\ \hline
\textbf{faster\_rcnn\_inception\_v2\_coco} & 17         & 28       \\ \hline
\textbf{mask\_rcnn\_inception\_v2\_coco}   & 13         & 25       \\ \hline
\end{tabular}
\end{center}
\caption{Tensorflow models performance (from \href{https://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/detection_model_zoo.md#coco-trained-models}{Tensorflow detection model zoo}). Note: the COCO mAP numbers here are evaluated on COCO 14 minival set using the \href{http://cocodataset.org/#detection-eval}{MSCOCO evaluation protocol}}
\end{table}
The reported running time in ms is done for 600x600 images (including all pre and post-processing). As the authors say, ``these timings depend highly on one's specific hardware configuration (performed using an Nvidia GeForce GTX TITAN X card) and should be treated more as relative timings in many cases". However, some characteristics of each model can be extracted in terms of speed and accuracy. As expected, the R-CNN models achieve better accuracy while performing a little slower with respect to SSD.

\subsubsection{SSD MobileNetV2}
This SSD implementation uses MobileNetV2 as backbone. As commented in \ref{mobilenet}, MobileNetV2 is a network proposed to work on mobile devices, this can be interesting to the project because of the hardware limits (it needs to work on CPU only).
\subsubsection{Faster R-CNN InceptionV2}
This region-based model was discussed in \ref{mobilenet}. The implementation uses InceptionV2 \cite{szegedy2016rethinking} as the feature extractor. This backbone follows the idea of its predecesor (InceptionV1) and adds two main ideas: reduce the representational bottleneck and use smart factorization methods. Refering to the first one, the intuition is that neural networks usually perform better when the convolutions do not alter the dimensions of the input in a drastical way (may cause loss of information). To solve this problem they expand the inception module making it wider (instead of deeper). Apart from that, the convolutions are made more efficient in terms of computational complexity. The authors propose factorizing the 5x5 convolution into two 3x3 convolution operations making it 2,78 times faster, among others.
\subsubsection{Mask R-CNN InceptionV2}
This state-of-the-art instance segmentation network is used as object detection network because it offers the bounding boxes locations, apart from the instance masks (Figure \ref{fig:maskrcnn_tests}). The implementation selected also makes use of InceptionV2 behind Mask R-CNN.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.5]{figures/maskrcnn_first_tests.png}
\caption{First tests with Mask R-CNN using live video}
\label{fig:maskrcnn_tests}
\end{center}
\end{figure} 
\subsection{Keras models}
The Keras network is a Keras port of the SSD model architecture introduced by Wei Liu et al. in \cite{liu2016ssd} from SSD-Keras\footnote{\href{https://github.com/pierluigiferrari/ssd_keras}{SSD-Keras}}. The repository offers pre-trained models and allows the model training from scratch. The base network architecture used is VGG (see \ref{mobilenet}). The pre-trained models used were trained on the PASCAL VOC dataset. The pre-trained models available include COCO and ILSVRC datasets but PASCAL was selected because COCO dataset was already used in the previous SSD model in Tensorflow. In the next table, it can be seen the reported performance made by the author:
\begin{table}[H]
\begin{center}
\begin{tabular}{|c|c|c|}
\hline
Model name               & Speed (FPS) & VOC2007 test mAP @ 0,5 \\ \hline
\textbf{SSD300\_VOC0712} & 39          & 77,5                   \\ \hline
\textbf{SSD500\_VOC0712} & 20          & 79,8                   \\ \hline
\end{tabular}
\end{center}
\caption{Keras models performance (from \href{https://github.com/pierluigiferrari/ssd_keras#performance}{SSD-Keras performance}). Note: evaluated using official Pascal VOC 2012 test server using an NVIDIA GeForce GTX 1070 mobile.}
\end{table}
The author affirms that the implementation performs slightly better than the original SSD implementation in Caffe\footnote{\href{https://github.com/weiliu89/caffe/tree/ssd}{SSD-Caffe}}.

\section{Tracker}
The Tracker module is the core of the project and therefore it is going to be explained more in deep.\\
As previously commented, the Tracker receives an input detection coming from the Network module and performs the multiobject tracking over a buffer of variable size following a tracking by detection scheme. With this hybrid tracker it is pretended to show the advantages of the tracking by detection with deep learning over a pure neural network tracking or a classic feature tracking.\\
The tracker can work in three operating regimes: \textit{slow, normal} and \textit{fast}. The calculation of this regime is performed using an internal buffer of FPS rates of size 3. The length selected is due to the fact that the tracker needs to respond quickly to changes in its velocity avoiding slowing down or speeding up in an excessive way. The tracker speed calculation is explained in Algorithm \ref{tracker_speed}. Three frame rate thresholds are established to distinguish between a tracker which is behaving in a normal way or a slower or a faster processing time. If the tracker is performing slower than normal the next three frames that needed to be tracked are discarded from the buffer in the \texttt{imageToTrack} function. In the other hand, if the tracker is going too fast it is slowed down in \texttt{getOutputImage} by waiting some frames to return the image to be output. With this mechanism the buffer does not increase or decrease too much in size allowing a more stable behavior and a good synchronization between the threads.

\begin{algorithmic}[H]
\begin{algorithm}
\State \textbf{Inputs:} averageFPS, lastFPSBuffer, trackerSlow, trackerFast, counterSlow, counterFast
\State \textbf{Output:} trackerSpeedMode
\Procedure{trackerSpeedMode}{}
\If {not 0 in $lastFPSBuffer$ and $averageFPS$ \textless 10}
    \State $counterSlow$ + 1
    \If {$counterSlow$ == 3}
        \State $counterSlow$ = 0
        \State $trackerSlow$ = True
    \EndIf
\ElsIf{not 0 in $lastFPSBuffer$ and 10 \textless $averageFPS$ \textless 25}
    \State $trackerSlow$ = False
    \State $trackerFast$ = False
\ElsIf{$averageFPS$ \textgreater 25 and $counterFast$ \textless 1}
    \State $counterFast$ + 1
\State $trackerFast$ = True
\EndIf  
\EndProcedure
\caption{Tracker speed mode}\label{tracker_speed}
\end{algorithm}
\end{algorithmic}

This dynamic calculation allows the tracking to behave differently depending on the operating regime in which it is on each instant. However, it will be seen in the next chapter that this regime is very depending on the tracker that is being used and its speed performance.\\
The tracking will be performed using already built tracking implementations of two libraries: OpenCV and dlib. This will also allow for a good comparative between them that can lead us to select the preferred option for the tracking algorithm. In the next subsection, the tested tracker implementations are discussed.
\subsection{OpenCV tracking}
OpenCV is known for the great variety of algorithms for which it provides implemented solutions, one of them is tracking. This libraries are included in the OpenCV extra modules (\texttt{opencv contrib}).\\
The tested trackers include \textit{BOOSTING, MIL, MEDIANFLOW, TLD, KCF, MOSSE} and \textit{CSRT} (in release order).
\\
The OpenCV trackers available in this project are now introduced:
\begin{itemize}
\item BOOSTING \cite{grabner2006real}: based on an online version of AdaBoost, the tracker is trained at runtime with positive and negative examples of the object to track. An initial bounding box needs to be provided by the user or other object detection algorithm. The classifier looks over the pixel neighborhood of a previous location to find the new location. The classifier is constantly updated with this new positives.
\item MIL \cite{babenko2009visual}: the \textit{Multiple Instance Learning} algorithm tries to solve the problem of learning an adaptive appearance model for object tracking. To achieve this, the authors train a discriminative classifier online to separate the object to track from the background, i.e.\ positive and negative examples are extracted from the frame (Figure \ref{fig:mil}). Similarly to the Boosting algorithm, the model searches inside of the window of the old location. It obtains a probability map with most probably new location of the object and updates the tracker model.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{figures/mil.png}
\caption{Updating a discriminative appearance model: (A) using a single positive image patch. (B) using several positive image patches. (C) using one positive bag of several image patches (from \cite{babenko2009visual})}
\label{fig:mil}
\end{center}
\end{figure}
\item MEDIANFLOW \cite{kalal2010forward}: the Median Flow algorithm introduced a novel method for tracking failure detection based on the Forward-Backward error. This  basically consists of perform the tracking forward and backward in time in a given frame and measure the discrepances between trajectories (see Figure \ref{fig:medianflow}). The authors affirm that this discrepances are highly correlated with real tracking failures.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{figures/medianflow.png}
\caption{The forward-backward error in Point 2 (from \cite{kalal2010forward})}
\label{fig:medianflow}
\end{center}
\end{figure}    
\item TLD \cite{kalal2011tracking}: in the original paper on which is based the implementation, the authors investigate the long-term object tracking and propose a novel framework that descompose this type of tracking into \textit{tracking, learning and detection} (TLD). The tracker based on Median Flow follows the object in every frame. The detector is composed by a patch variance module, followed by an ensemble classifier and finally a Nearest Neighbor classifier. The function of this detector is to correct the tracker if necessary. The learning step estimates the errors of the detector and updates it using a novel method called \textit{P-N learning}.
\item KCF \cite{henriques2012exploiting}: the \textit{Kernelized Correlation Filter} is a tracking framework that utilizes properties of circulant matrix to enhance the processing speed. The authors observed that the translated and scaled patches used to train discriminative classifiers contain redundancies and the resulting data matrix from this patches is circulant. With kernel regression as classification method they derive the KCF tracking.
\item MOSSE \cite{bolme2010visual}: correlation filters can track complex objects in common tracking scenarios that may include rotations, occlusions or other distractions at high frame rates. The \textit{Minimum Output Sum of Squared Error} filter is another type of correlation filter. Filter based trackers model the appearance of objects using filters trained in example images (Figure \ref{fig:mosse}). With a given initial target in the first frame the tracking and the filter training start to work together. The idea behind MOSSE is an optimization problem, given a set of training images $f_i$ and training outputs $g_i$, MOSSE finds a filter \textit{H} that minimizes the sum of squared error between the actual output of the convolution and the desired output of the convolution (see Formula \ref{eq:conv_mosse}).
\begin{equation}
\min_{H^*}\sum_{i}\left|F_i \odot H^* - G_i\right|^2
\label{eq:conv_mosse}
\end{equation}

\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.35]{figures/mosse_filter.png}
\caption{Comparison of the output peaks produced by different correlation filters (from \cite{bolme2010visual})}
\label{fig:mosse}
\end{center}
\end{figure} 

\item CSRT \cite{lukezic2017discriminative}: the CSRT tracker is based on the paper \textit{Discriminative Correlation Filter with Channel and Spatial Reliability}. Here the authors introduce the channel and spatial reliability concepts to DCF tracking to improve the filter update and the tracking process (Figure \ref{fig:csrt}). The spatial information is used to restrict the searching to the parts suitable for tracking. In the other hand, the channel information aims to reduce the noise of the weighted-averaged filter response.
\begin{figure}[H]
\begin{center}
\includegraphics[scale=0.4]{figures/csrt.png}
\caption{Overview of the CSR-DCF approach (from \cite{lukezic2017discriminative})}
\label{fig:csrt}
\end{center}
\end{figure} 
\end{itemize}

In the next chapter, some experiments are performed to put into practice the advantages and disadvantages of each OpenCV tracker. Furthermore, the dlib tracking will also be discussed allowing for a comparison between all the proposals and allowing us to select the best tracker option.
\subsection{dlib tracking}
In chapter 2, the dlib library was introduced as a set of independent software components that provide different utilities, one of them is tracking. The \texttt{dlib.correlation\_tracker} is going to be used for this project. As the name indicates, it is another implementation of a correlation filter for tracking which are widely used. This tool is an implementation of the method described in \cite{danelljan2014accurate}.\\ In the proposed solution, the authors are centered in solving the challenging problem of handling large scale variations in visual object tracking. They propose a method for a robust scale estimation in a tracking by detection framework, as it is the case in this project. To do so the learning of discriminative correlation filters is based on a scale pyramid representation.
